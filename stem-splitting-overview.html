<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Audio Stem Splitting Pipeline — Technical Overview</title>
<style>
/* Typography */
@import url('https://fonts.googleapis.com/css2?family=Fraunces:ital,opsz,wght@0,9..144,300..900;1,9..144,300..900&family=Source+Code+Pro:wght@400;600&family=Source+Sans+3:wght@400;600&family=Source+Serif+4:ital,wght@0,400;0,600;1,400&display=swap');

:root {
  /* Fonts */
  --font-display: 'Fraunces', Georgia, serif;
  --font-body: 'Source Serif 4', Georgia, serif;
  --font-body-sans: 'Source Sans 3', system-ui, sans-serif;
  --font-mono: 'Source Code Pro', monospace;

  /* Colors - Primitives */
  --color-ink: #2c2825;
  --color-ink-soft: #5c5550;
  --color-ink-muted: #8a8580;
  --color-cream: #f7f5f2;
  --color-paper: #faf9f7;
  --color-linen: #f0ede8;
  --teal-standard: #4b7c8c;
  --teal-deep: #1a3a40;
  --teal-pale: #b8d4dc;
  --ember-terracotta: #c4704b;

  /* Semantic Tokens */
  --color-text: var(--color-ink);
  --color-text-soft: var(--color-ink-soft);
  --color-text-muted: var(--color-ink-muted);
  --color-background: var(--color-paper);
  --color-surface: var(--color-cream);
  --color-surface-raised: var(--color-linen);
  --color-accent: var(--teal-standard);
  --color-accent-deep: var(--teal-deep);
  --color-accent-pale: var(--teal-pale);
  --color-callout: var(--ember-terracotta);

  /* Spacing */
  --space-xs: 8px;
  --space-sm: 12px;
  --space-md: 20px;
  --space-lg: 32px;
  --space-xl: 48px;
  --space-xxl: 64px;

  /* Shadows */
  --shadow-soft: 0 2px 8px hsla(25, 40%, 30%, 0.08);
  --shadow-lifted: 0 4px 16px hsla(25, 40%, 30%, 0.12);
}

* {
  box-sizing: border-box;
}

body {
  font-family: var(--font-body);
  font-size: 16px;
  line-height: 1.6;
  color: var(--color-text);
  background: var(--color-background);
  margin: 0;
  padding: var(--space-lg) var(--space-md);
}

.container {
  max-width: 820px;
  margin: 0 auto;
}

/* Typography */
h1, h2, h3, h4 {
  font-family: var(--font-display);
  line-height: 1.2;
  margin: 0 0 var(--space-md) 0;
  font-weight: 600;
  color: var(--color-text);
}

h1 {
  font-size: 42px;
  margin-bottom: var(--space-sm);
  font-variation-settings: 'opsz' 144;
}

.subtitle {
  font-family: var(--font-body-sans);
  font-size: 18px;
  color: var(--color-text-soft);
  margin: 0 0 var(--space-xl) 0;
  font-weight: 400;
}

h2 {
  font-size: 32px;
  margin-top: var(--space-xxl);
  padding-bottom: var(--space-sm);
  border-bottom: 2px solid var(--color-linen);
  font-variation-settings: 'opsz' 96;
}

h3 {
  font-size: 24px;
  margin-top: var(--space-lg);
  color: var(--color-ink-soft);
}

h4 {
  font-size: 18px;
  margin-top: var(--space-md);
  color: var(--color-ink-soft);
}

p {
  margin: 0 0 var(--space-md) 0;
}

ul, ol {
  margin: 0 0 var(--space-md) 0;
  padding-left: var(--space-lg);
}

li {
  margin-bottom: var(--space-xs);
}

code {
  font-family: var(--font-mono);
  font-size: 14px;
  background: var(--color-linen);
  padding: 2px 6px;
  border-radius: 3px;
  color: var(--color-accent-deep);
}

pre {
  font-family: var(--font-mono);
  font-size: 14px;
  line-height: 1.5;
  background: var(--color-linen);
  padding: var(--space-md);
  border-radius: 8px;
  border: 1px solid var(--color-surface-raised);
  overflow-x: auto;
  margin: 0 0 var(--space-md) 0;
  box-shadow: var(--shadow-soft);
}

pre code {
  background: none;
  padding: 0;
  color: var(--color-text);
}

/* Tables */
table {
  width: 100%;
  border-collapse: collapse;
  margin: var(--space-md) 0 var(--space-lg) 0;
  background: var(--color-surface);
  border-radius: 8px;
  overflow: hidden;
  box-shadow: var(--shadow-soft);
}

thead {
  background: var(--color-accent);
  color: white;
}

th, td {
  text-align: left;
  padding: var(--space-sm) var(--space-md);
  border-bottom: 1px solid var(--color-linen);
}

th {
  font-family: var(--font-body-sans);
  font-weight: 600;
  font-size: 14px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

td {
  font-size: 15px;
}

tbody tr:last-child td {
  border-bottom: none;
}

tbody tr:hover {
  background: var(--color-cream);
}

td code {
  font-size: 13px;
}

/* Callout Boxes */
.callout {
  background: var(--color-cream);
  border-left: 4px solid var(--color-callout);
  padding: var(--space-md) var(--space-md) var(--space-md) var(--space-lg);
  margin: var(--space-lg) 0;
  border-radius: 6px;
  box-shadow: var(--shadow-soft);
}

.callout h4 {
  margin-top: 0;
  color: var(--color-callout);
  font-size: 16px;
  font-family: var(--font-body-sans);
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.callout p:last-child {
  margin-bottom: 0;
}

.callout ul {
  margin-bottom: 0;
}

/* Flow Diagram */
.flow-step {
  background: var(--color-surface);
  padding: var(--space-md);
  margin: var(--space-sm) 0;
  border-radius: 8px;
  border-left: 4px solid var(--color-accent);
  box-shadow: var(--shadow-soft);
}

.flow-step-header {
  font-family: var(--font-body-sans);
  font-weight: 600;
  color: var(--color-accent-deep);
  margin-bottom: var(--space-xs);
  font-size: 15px;
}

.flow-step-tool {
  font-family: var(--font-mono);
  font-size: 14px;
  color: var(--color-accent);
  margin-bottom: var(--space-xs);
}

.flow-step-description {
  color: var(--color-text-soft);
  font-size: 15px;
  margin: 0;
}

/* Folder Structure */
.folder-structure {
  background: var(--color-linen);
  padding: var(--space-md);
  border-radius: 8px;
  border: 1px solid var(--color-surface-raised);
  margin: var(--space-md) 0;
  font-family: var(--font-mono);
  font-size: 14px;
  line-height: 1.8;
  box-shadow: var(--shadow-soft);
}

.folder-structure .folder {
  color: var(--color-accent);
  font-weight: 600;
}

.folder-structure .comment {
  color: var(--color-text-muted);
}

/* Header */
header {
  border-bottom: 3px solid var(--color-linen);
  padding-bottom: var(--space-lg);
  margin-bottom: var(--space-xl);
}

/* Print Styles */
@media print {
  body {
    background: white;
    padding: 0;
  }

  .callout {
    break-inside: avoid;
  }

  table {
    break-inside: avoid;
  }

  h2, h3 {
    break-after: avoid;
  }
}

/* Responsive */
@media (max-width: 600px) {
  body {
    padding: var(--space-md) var(--space-sm);
  }

  h1 {
    font-size: 32px;
  }

  h2 {
    font-size: 26px;
  }

  table {
    font-size: 13px;
  }

  th, td {
    padding: var(--space-xs) var(--space-sm);
  }
}
</style>
</head>
<body>

<div class="container">

<header>
  <h1>Audio Stem Splitting Pipeline</h1>
  <p class="subtitle">A local-first workflow for producing multi-level practice tracks from YouTube source audio, built with Claude Code</p>
</header>

<section>
  <h2>What This Is</h2>

  <p>This is a technical communication document describing a local-first audio processing pipeline for youth musical theater. It takes YouTube source audio, separates vocals from instruments using AI, then produces practice tracks at multiple vocal levels. Students access the final recordings via Soundslice, an interactive sheet music player.</p>

  <p>The pipeline runs entirely on a local Mac (M-series), processes a typical show in under an hour, and produces four versions of each song: full mix, half vocals, ghost vocals, and karaoke. Every step is automated, idempotent, and logged.</p>

  <p>The entire pipeline was built collaboratively with Claude Code — an LLM-assisted workflow that turned musical theater production needs into a robust automation system.</p>
</section>

<section>
  <h2>The Core Tool: Demucs</h2>

  <p><strong>Demucs</strong> is Meta's open-source AI audio source separation tool. It runs locally on Mac hardware (M-series processes a 3-minute song in about 1-2 minutes) and produces surprisingly clean separations between vocals and instrumentals.</p>

  <h3>Installation</h3>

  <pre><code># Install dependencies via Homebrew
brew install ffmpeg yt-dlp

# Create a Python virtual environment
python3 -m venv ~/audio-venv
source ~/audio-venv/bin/activate

# Install Demucs
pip install demucs</code></pre>

  <h3>Key Command</h3>

  <pre><code>demucs --two-stems vocals -n htdemucs "source.wav"</code></pre>

  <ul>
    <li><code>--two-stems vocals</code> — Outputs two files: <code>vocals.wav</code> and <code>no_vocals.wav</code>. This is almost always what you want for practice tracks (simpler than 4-stem separation into drums/bass/other/vocals).</li>
    <li><code>-n htdemucs</code> — Specifies the Hybrid Transformer model, a good default for most music.</li>
    <li><code>-n htdemucs_ft</code> — Fine-tuned variant, slower but better on problem tracks (heavily orchestrated, choral, or dense mixes).</li>
  </ul>

  <h3>Alternatives Worth Knowing</h3>

  <table>
    <thead>
      <tr>
        <th>Tool</th>
        <th>Notes</th>
        <th>Cost</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Lalal.ai Phoenix</strong></td>
        <td>Web-based, extremely clean on orchestral music</td>
        <td>Subscription</td>
      </tr>
      <tr>
        <td><strong>Moises.ai</strong></td>
        <td>Musician-focused app with pitch/tempo control</td>
        <td>Freemium</td>
      </tr>
      <tr>
        <td><strong>Logic Pro</strong></td>
        <td>Built-in stem separation (adequate, not exceptional)</td>
        <td>One-time</td>
      </tr>
      <tr>
        <td><strong>Demucs</strong></td>
        <td>Open-source, scriptable, runs locally</td>
        <td>Free</td>
      </tr>
    </tbody>
  </table>

  <p>For a production pipeline where you need repeatability, logging, and integration with other tools, Demucs is the best choice.</p>
</section>

<section>
  <h2>The Pipeline</h2>

  <p>The full pipeline consists of 7 steps (numbered 0-6). Each step is idempotent — re-running the pipeline skips completed work and only processes what's new or changed.</p>

  <div class="flow-step">
    <div class="flow-step-header">Step 0: Environment Check</div>
    <div class="flow-step-tool">bash</div>
    <div class="flow-step-description">Verifies all required tools are installed and accessible (yt-dlp, ffmpeg, demucs, rubberband, sox). Exits early if anything is missing.</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 1: Download Source Audio</div>
    <div class="flow-step-tool">yt-dlp</div>
    <div class="flow-step-description">Downloads audio from YouTube URLs as high-quality WAV files. Takes ~1-2 minutes total for a typical show.</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 2: Stem Separation</div>
    <div class="flow-step-tool">Demucs</div>
    <div class="flow-step-description">Splits each song into vocals.wav + no_vocals.wav using AI source separation. ~1-2 minutes per song on M-series Mac.</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 3: Pitch Shifting (Optional)</div>
    <div class="flow-step-tool">Rubber Band</div>
    <div class="flow-step-description">Pitch shifts vocals and instrumentals separately to match the performer's range. ~10-20 seconds per stem. Formant preservation enabled automatically for large shifts (3+ semitones).</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 4: Cut and Splice (Optional)</div>
    <div class="flow-step-tool">FFmpeg</div>
    <div class="flow-step-description">Trims intros, removes sections, or reorders segments. Millisecond-precise. Takes seconds per operation.</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 5: Lead/Backing Separation (Optional)</div>
    <div class="flow-step-tool">audio-separator</div>
    <div class="flow-step-description">Further splits vocals into lead vs backing vocals. Useful for ensemble pieces. ~1 minute per song.</div>
  </div>

  <div class="flow-step">
    <div class="flow-step-header">Step 6: Normalization and Export</div>
    <div class="flow-step-tool">FFmpeg</div>
    <div class="flow-step-description">Normalizes to -14 LUFS (consistent perceived loudness) and exports final MP3s. Takes seconds per file.</div>
  </div>

  <p>A typical 12-song show processes in under an hour, with most of that time spent on stem separation (Step 2).</p>
</section>

<section>
  <h2>The Delivery Layer</h2>

  <p>After stems are produced, a separate Python script (<code>catalog.py</code>, stdlib only) automates uploading to Soundslice via their API. It creates <strong>four recordings per song</strong>, each with a different vocal level:</p>

  <table>
    <thead>
      <tr>
        <th>Recording</th>
        <th>Vocal Level</th>
        <th>Purpose</th>
        <th>How It's Made</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Full Mix</strong></td>
        <td>100%</td>
        <td>Learn the song, hear everything</td>
        <td>Original vocals.wav + no_vocals.wav</td>
      </tr>
      <tr>
        <td><strong>Half Vox</strong></td>
        <td>50%</td>
        <td>Practice with vocal support</td>
        <td>no_vocals.wav + vocals.wav at 0.5 volume</td>
      </tr>
      <tr>
        <td><strong>Ghost Vox</strong></td>
        <td>10%</td>
        <td>Near-solo, faint guide</td>
        <td>no_vocals.wav + vocals.wav at 0.1 volume</td>
      </tr>
      <tr>
        <td><strong>Karaoke</strong></td>
        <td>0%</td>
        <td>Performance mode</td>
        <td>no_vocals.wav only</td>
      </tr>
    </tbody>
  </table>

  <h3>Mixing Command</h3>

  <p>The half and ghost mixes are generated using FFmpeg's <code>amix</code> filter with a critical flag:</p>

  <pre><code>ffmpeg -i no_vocals.wav -i vocals.wav \
  -filter_complex "[1:a]volume=0.5[v];[0:a][v]amix=inputs=2:duration=first:normalize=0" \
  half_vox.wav</code></pre>

  <div class="callout">
    <h4>Critical: normalize=0</h4>
    <p>Without <code>normalize=0</code>, FFmpeg automatically reduces the instrumental volume to "make room" for the vocals. This results in the instrumentals sounding too quiet in half/ghost mixes. The flag tells FFmpeg: "Don't auto-adjust levels, just mix what I give you."</p>
  </div>
</section>

<section>
  <h2>Folder Structure</h2>

  <p>The pipeline organizes work into a clear, sequential folder structure. Each step reads from the previous step's output and writes to its own folder.</p>

  <div class="folder-structure">
<span class="folder">Production/</span>
  <span class="folder">01-Source/</span>          <span class="comment"># Downloaded WAV files from YouTube</span>
  <span class="folder">02-Stems/</span>           <span class="comment"># Demucs output (vocals.wav + no_vocals.wav per song)</span>
  <span class="folder">02-Stems-MP3/</span>       <span class="comment"># Cached MP3 conversions for quick review</span>
  <span class="folder">03-Shifted/</span>         <span class="comment"># Pitch-shifted variants (optional)</span>
  <span class="folder">04-Cut/</span>             <span class="comment"># Trimmed/spliced versions (optional)</span>
  <span class="folder">05-Lead-Backing/</span>    <span class="comment"># Lead vs backing vocal separation (optional)</span>
  <span class="folder">06-Final/</span>           <span class="comment"># Normalized exports ready for upload</span>
  <span class="folder">logs/</span>               <span class="comment"># Per-step, per-song execution logs</span>
  </div>

  <p>This structure makes it easy to:</p>
  <ul>
    <li>Spot-check the output of any individual step</li>
    <li>Re-run a single step without redoing earlier work</li>
    <li>Debug issues by reading the logs for a specific song and step</li>
    <li>Archive or delete intermediate files once the final versions are confirmed</li>
  </ul>
</section>

<section>
  <h2>Hard-Won Lessons</h2>

  <p>These are the kinds of details you only learn after processing a few dozen songs and noticing something sounds wrong.</p>

  <div class="callout">
    <h4>Pitch Shift Stems Separately, Not the Mix</h4>
    <p>Always pitch shift the vocals and instrumentals independently, then recombine them. Shifting the full mix sounds noticeably worse — artifacts that are subtle in one stem become obvious when both are shifted together.</p>
  </div>

  <div class="callout">
    <h4>Two-Pass Loudnorm</h4>
    <p>FFmpeg's <code>loudnorm</code> filter requires two passes to hit its target accurately. Pass 1 measures the audio and outputs correction values. Pass 2 applies those values. Single-pass loudnorm consistently overshoots its target by 1-2 LUFS.</p>
    <pre><code># Pass 1: Measure
ffmpeg -i input.wav -af loudnorm=print_format=json -f null -

# Pass 2: Apply (using measured_I, measured_LRA, etc.)
ffmpeg -i input.wav -af loudnorm=I=-14:LRA=7:measured_I=-18.2:... output.wav</code></pre>
  </div>

  <div class="callout">
    <h4>Formant Preservation for Big Shifts</h4>
    <p>Rubber Band's <code>--formant</code> flag prevents chipmunk (shifting up) or monster (shifting down) effects. Auto-enable it for shifts of 3 or more semitones. Below that threshold, it's usually not necessary.</p>
  </div>

  <div class="callout">
    <h4>Two-Stem Mode Is Almost Always What You Want</h4>
    <p>Demucs can separate audio into 4 stems (vocals, drums, bass, other). This sounds impressive but for practice tracks you just need "voice" vs "not voice." Two-stem mode (<code>--two-stems vocals</code>) is faster, simpler to work with, and sounds better when recombined.</p>
  </div>

  <div class="callout">
    <h4>Idempotency Saves Hours</h4>
    <p>Every step checks whether its output already exists before running. This means you can re-run the entire pipeline after adding one new song, and it will only process that song — skipping the 11 songs it already completed. Build this in from the start.</p>
  </div>
</section>

<section>
  <h2>Replicating This With an LLM</h2>

  <p>The entire pipeline was built collaboratively with Claude Code. Here's what made that process work well:</p>

  <h3>Key Context to Provide</h3>
  <ul>
    <li><strong>What tools you have installed.</strong> Claude Code can check your system, but starting with "I have ffmpeg, yt-dlp, and demucs installed" saves time.</li>
    <li><strong>Your folder structure.</strong> Show the LLM where source files live and where outputs should go.</li>
    <li><strong>Your end deliverable.</strong> "I need four MP3 files per song, normalized to -14 LUFS, uploaded to Soundslice" is much clearer than "I need practice tracks."</li>
    <li><strong>Your constraints.</strong> "No pip dependencies in the upload script" or "Must run on Mac M1" shapes the solution space early.</li>
  </ul>

  <h3>Why This Works Well With LLMs</h3>
  <p>The pipeline is built entirely on CLI tools (bash, ffmpeg, demucs, rubberband). These have well-documented interfaces that LLMs understand deeply. You're not asking the LLM to invent new algorithms — you're asking it to orchestrate existing tools in a sensible sequence.</p>

  <h3>Iteration Pattern</h3>
  <ol>
    <li><strong>Start with one song, one step.</strong> Get Step 2 (stem separation) working perfectly before moving to Step 3.</li>
    <li><strong>Test with real audio.</strong> Synthetic examples don't reveal issues like DC offset, clipping, or phase problems.</li>
    <li><strong>Listen critically.</strong> If something sounds wrong, describe what you hear. "The vocals sound thin in the half mix" led directly to discovering the <code>normalize=0</code> issue.</li>
    <li><strong>Build idempotency early.</strong> Once you have 3-4 steps working, the time savings from re-running only changed steps becomes significant.</li>
  </ol>
</section>

<section>
  <h2>Tools Reference</h2>

  <p>A compact reference for all tools used in the pipeline.</p>

  <table>
    <thead>
      <tr>
        <th>Tool</th>
        <th>Purpose</th>
        <th>Install</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Demucs</strong></td>
        <td>AI vocal/instrumental separation</td>
        <td><code>pip install demucs</code> (in a venv)</td>
      </tr>
      <tr>
        <td><strong>yt-dlp</strong></td>
        <td>YouTube audio download</td>
        <td><code>brew install yt-dlp</code></td>
      </tr>
      <tr>
        <td><strong>FFmpeg</strong></td>
        <td>Audio conversion, mixing, normalization, cutting</td>
        <td><code>brew install ffmpeg</code></td>
      </tr>
      <tr>
        <td><strong>Rubber Band</strong></td>
        <td>High-quality pitch shifting with formant preservation</td>
        <td><code>brew install rubberband</code></td>
      </tr>
      <tr>
        <td><strong>audio-separator</strong></td>
        <td>Lead/backing vocal separation (optional)</td>
        <td><code>pip install audio-separator[cpu]</code></td>
      </tr>
      <tr>
        <td><strong>SoX</strong></td>
        <td>Audio utilities (optional, for advanced effects)</td>
        <td><code>brew install sox</code></td>
      </tr>
    </tbody>
  </table>

  <h3>Minimum System Requirements</h3>
  <ul>
    <li><strong>macOS</strong> 11.0+ (Intel or Apple Silicon)</li>
    <li><strong>Python</strong> 3.8+</li>
    <li><strong>Disk space:</strong> ~500MB per song for intermediate files (can be deleted after final export)</li>
    <li><strong>RAM:</strong> 8GB minimum, 16GB recommended for large orchestral files</li>
  </ul>
</section>

<footer style="margin-top: var(--space-xxl); padding-top: var(--space-lg); border-top: 2px solid var(--color-linen); color: var(--color-text-muted); font-size: 14px;">
  <p>Pipeline developed for Actors Garden youth musical theater program. Built collaboratively with Claude Code, February 2026.</p>
</footer>

</div>

</body>
</html>